This section explains how data was collected, how the different visualizations were coded, and how the candidates were tested.

\subsection{Data Collection Pipeline}
This section will go into further detail what data was collected from Twitter and how this data was collected, prepared, and stored.

\subsubsection{Fetching the data from Twitter}

The application for collecting the tweets was written in Python 3 (\cite{10.5555/1593511}), the Twitter API was accessed using the package \emph{Tweepy} (\cite{roesslein2020tweepy}). Using the streaming capabilities of Tweepy, all tweets in German containing one or more of the keywords \begin{verbatim}
    Corona, Covid-19, covidioten, distancing
\end{verbatim}
were collected. From the returned Tweet Objects\footnote{The documentation on what data a Tweet Object contains can be found here: https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object}, only a few data points were kept that seemed interesting before the data collection started. This has mainly ethical reasons (\cite{richards2014big}). The author did not want to collect excessive personal data without having a good reason to, even though this has become the norm for big data over the last decade. The following data points of a tweet object get processed further and eventually saved to a database:

\begin{itemize}
\item \textbf{The full text of the tweet.} This is mainly used for the sentiment analysis and other natural language processing-tasks.
\item \textbf{The tweet's time-stamp.}
\item \textbf{The ID of the tweet.} This makes it easier to check for duplicates in the data base. 
\item \textbf{The user name of the tweet author.} As discussed above, opinion leadership is a phenomenon also on Twitter. Collecting the user names allows for later analysis, e.g., which users tweet the most.
\item \textbf{The user's self-description.} Users on Twitter can enter a short description, the so-called \emph{bio}.
\item \textbf{The user's friend count.} This shows how many other accounts the user in question is following.
\item \textbf{The user's follower count.} This shows how many other accounts follow the user in question. \footnote{The definitions for a user's friend count and follower count can be found here: https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/overview}
\item \textbf{The country where the tweet was sent.} Twitter also offers coordinate-based geolocations, but choosing the country instead seemed to honour the user's privacy more.
\item \textbf{Whether the account is verified or not.}
\item \textbf{The source of the tweet.} This shows if the tweet came from the Web-App, a desktop app, a mobile app, a 3rd party-application or other sources.
\item \textbf{The calculated sentiment of the tweet.} The score gets calculated during the pre-processing of the tweet. Later sections will go into more detail on how the sentiment is calculated.
\end{itemize}

%TODO: Add figure how the data was collected (it's in miro)

Twitter's \emph{Streaming API} was used as it gives the most complete data set on Twitter's free tier (\cite{bruns2014}). This meant, however, that the application had to run 24/7: the streaming API streams the data as it comes in and doesn't save it on Twitter's network. To ensure constant availability, the python application that connects to the streaming API and processes the incoming tweets runs in a Docker Container (\cite{merkel2014docker}) on Google's App Engine (\cite{google2020}). This makes it easy to set up a server-less infrastructure with high availability.

\subsubsection{Data processing during the collection}
Every time the Streaming API sent a tweet to the app collecting the data, a sentiment score for the tweet text was calculated. For this, the Natural Language Toolkit (NLTK) was used (\cite{loper2002nltk}). NLTK uses a dictionary-based approach for sentiment analysis. This means that the tool kit relies on a corpus that is coded with different sentiment scores (\cite{haselmayer2017sentiment}). Compared with machine learning-approaches, this approach high precision, but low recall (\cite{soroka2015}). This means that the polarity of a sentiment as computed by NLTK is usually correct; however, many text that do carry a specific sentiment are not recognized and are as suched marked as texts with neutral sentiments.
A machine-learning based approach for calculating sentiments was also considered. However, the tool would have required a server running a Java-application. This would have meant a second instance of Google AppEngine, doubling the costs. Considering that the goal of this study is to test how laymen work with dataset, and not a scientific analysis of the discussion about the corona virus in the German twittersphere, the dictionary-based approach was chosen to keep down the costs.

\subsubsection{Storing the pre-processed tweets}
Tweets collected via the Streaming API are bundled to batches of 100. These batches then get published as a message to a \emph{Pub/Sub Topic}. According to Google, \say{Pub/Sub is an asynchronous messaging service that decouples services that produce events from services that process events} (\cite{google2020a}). In this case, collecting the tweets gets decoupled from writing the tweets to a data base. This approach has multiple advantages. For one, the database connection does not need to be open constantly. This makes the pipeline more robust: if the database connection is closed---for whatever reason---the messages published to the pub/sub topic is saved for up to 30 days. Once the database connection is re-established, the messages get ingested and the tweets are added to the data base. It also allowed for some early experimentation with the table schema for the SQL table. Pub/Sub could be configured in a way that already acknowledged messages stay in the system and can be fetched again. This meant that even though the table schema was constantly reset in the first couple of days of data collection, the data was not lost and could simply be re-fetched again.
%TODO: More reasons why Pub/Sub is better than a direct database connection? I think it's better because API-limits for BigQuery are not hit as quickly if I batch-write the tweets.

The tweets are stored in a \emph{Google BigQuery} database, a server-less data warehouse that allows fast analysis of large amounts of data (\cite{google2020b}). \emph{Google Firebase} was also considered as data storage (\cite{google2020c}). As Firebase is a NoSQL database, queries would have been easier to write: Instead of using SQL, standard Javascript would have been used. This would result in easier queries as Javscript-features like method chaining would have been available. Seeing that the SQL-based BigQuery is better equipped to handle large amounts of data, this solution was chosen in the end. This turned out to be a good choice considering that the final dataset was almost 1GB big and that during an average testing session the data set was queried around 5 times. However, using BigQuery meant that the queries needed to be written in SQL, a language with complicated semantics (\cite{slutz1998massive}).

BigQuery was chosen over other SQL databases, like SQLite oder Postgres, because it is also a Google service like AppEngine. This meant that authentication between the collection script and the database did not have to be implemented. BigQuery can also be directly connected to a Pub/Sub-Topic using Google DataFlow. However, as the data were additionally processed, saving the data was handled in a dedicated script. 

\subsection{Visualizing the data}
To visualize the data from the streaming API, both Python-libraries like ggplot (\cite{wickham2016}) and JavaScript-Libraries like D3 (\cite{bostock}) and VegaLite (\cite{uwidl}) were evaluated. To make the prototype as accessible as possible, the choice was made to use a library based on the JavaScript web-technology. A web page is more accessible to laymen than a Python-based app. And even though Python scripts, including visualizations, can be embedded easily on the web now, choosing a web technology seemed like the right approach.

Ultimately, D3 was chosen as a visualization framework. VegaLite is an opinionated framework which chooses many defaults for the user, while D3 is unopinionated. This means that the visualizations are harder to code but more versatile.

Early prototypes of the tool used a Svelte-based web page to show the visualizations. The final tool, however, was written in Observable\footnote{https://observablehq.com}. Observable is a JavaScript-based notebook that, similarly to Jupyter Notebooks, combine code, output, and explanatory text. The notebooks allowed for quick iteration of the code, while its web-based nature also satisfied the requirement of being easily accessible without the need to download and install additional software.

\subsubsection{Building the visualizations}
For the tested prototype, two different visualizations were built: one which shows the volume of tweets over time, and one that shows the sentiment of tweets over time. Both visualizations shared a set of filters:
\begin{itemize}
    \item A word-filter which allows users to search the data base for tweets including a specific word
    \item A toggle to include retweets in the data set
    \item A toggle to include neutral tweets in the data set
\end{itemize}

