\section{Introduction}
"We also hope it might change your mind about what journalism could look like. We believe journalism doesnâ€™t always have to be a one-way conversation in which we present words for you to read. We also want to give you the tools to hold institutions in your life accountable for their choices." (\cite{angwinMakingPrivacyPersonal2020}) -> use this as explanation/motivation

Structure
\begin{itemize}
    \item humans have always communicated
    \item over the centuries, communication grew. From small, tight-knit communities, to larger communities, to one-sided mass-media communication, to globe-spanning social networks, allowing everyone with internet access to communicate with each other in real-time
    \item This opened communication empowered people to look up information for themselves, rather than having to rely on a single expert in their community. However, the internet is riddled with information with bad quality. This can lead to skepticism towards scientifically proven concepts and spread misinformation (\cite{krimskyRiskCommunicationInternet2007}). %TODO: find more recent source?
    \item The problem is: these kinds of contents often drive engagement. As the big players---Google, Facebook, Twitter---make money off of this engagement, it is in their interest that users see as many engagement-driving contents as possible. For this, they use recommendation algorithms.
    \item These algorithms lead to the so-called \emph{filter bubble-effect} which enforces the opinion a user already has (\cite{pariser2011filter}), can lead to social problems. Discussions can become less diverse and everyone thinks they are right.
    \item Currently, there is no way to find out reliably what Twitter users think inside of Twitter. One way to battle this problem is Big Data (\cite{crawfordCriticalQuestionsBig2012}). Using this approach could give users a birds-eye view of the network instead of a tailored experience created by recommendation algorithms.
    \item As its name suggests, Big Data relies on huge datasets to give meaningful results. Generating these datasets manually is a virtually impossible challenge.
    \item However, Twitter allows the automated collection of tweets via their APIs. This means that collection can run fully-automated over a prolonged time. Manual analysis of the collected data is not an option as the dataset is too big.
    \item Natural Language Processing can help to summarize these tweets in a way that makes manual coding of every tweet obsolete
    \item A good way to make such Big Data sets accessible to laymen is data visualization (\cite{donalekImmersiveCollaborativeData2014}). This, in combination with NLP, can help users to understand the data set on a meta-level, and not on the level of the individual tweets.
\end{itemize}

The goal of this thesis is two-fold: is it possible to automatically collect all German tweets to a specific topic and store them in a way that they can be analyzed? And can this data be processed and visualized in a way that allows laymen to gather information from this data?

\begin{itemize}
    \item For this, an app that automatically collects tweets about the ongoing corona epidemic was written.
    \item Then, interactive visualizations for this data were programmed
    \item Lastly, these visualizations were tested with laymen who had little to no experience working with Big Data sets.
\end{itemize}