\section{Related work}
This part reviews and compiles work that has already been done on Twitter analysis, big data, and visualization.

\subsection{Twitter as microblogging platform}
The microblogging platform \emph{Twitter} allows users to post messages that are up to 280 characters long, so-called \emph{tweets}. Users can follow other users; each user's timeline is then populated by tweets of people they follow (\cite{thimmTwitterAlsWahlkampfmedium2012}). Accounts can also be verified. This \say{lets people know that an account of public interest is authentic} (\cite{twitterinc.VerifiedAccounts}).

Seeing that Twitter users share their opinions on many topics openly, the platform offers a good data source for researchers to assess the public's opinion (\cite{pak2010twitter}, \cite{pfaffenberger2016twitter}, \cite{broniatowski2014twitter}). Twitter also has a very diverse audience: Politicians and journalists are using the platform the same way as celebrities or regular users (\cite{pak2010twitter}), which lets researchers collect data from different social groups and groups with diverse interests.

As opposed to other social networks, like Facebook, users can view tweets by all users instead of only by those users who are in their network (\cite{parkDoesTwitterMotivate2013}). While Facebook focuses on building a network of real-life friends and keeping in touch with them, Twitter nudges its users to voice their opinions publicly by choosing default settings for the user that supports this kind of usage. The Twitter page of a user and the page of a multi-national business conglomerate share the same functionality from the beginning, while Facebook differentiates between personal pages and public-facing pages.

Twitter is a medium that is widely used by the 'ordinary public', which means that people without celebrity status can use it as a platform to find an audience. A 2011 study conducted in South Korea examined the most popular 1\% of tweets. The researchers found out that the majority of those tweets were sent by people without celebrity status (\cite{chang2011structure}, as cited in \cite{parkDoesTwitterMotivate2013}).

Thus, Twitter is also a suitable survey tool. Collecting data from Twitter eliminates problems that classical surveys have, like low sample size or selection bias (\cite{takabeTwitterSurveyTool2016}). The low sample size is not a problem because of the high number of Twitter users, which was 330,000,000 by the beginning of 2019 (\cite{TwitterMonthlyActive}). The selection bias is circumvented because researchers are no longer active recruiters who look for participants. They are almost passive consumers of already existing data instead. Nevertheless, one has to keep in mind that community of Twitter users is not necessarily representative, as internet access, affinity to social media and so forth varies. %TODO: Add TAM or similar source here?

Because of Twitter's unique ability to reflect the thought processes of individuals, some scholars say that Twitter should be documented and archived as a historical record (\cite{risse2014documenting}). Twitter documents today's society in great detail---and, more importantly, individuals document their own experiences. This is a stark contrast to (historic) documentation in the past, where no direct documentation happened. There was always an intermediate between the individual who experienced something and the documentation (\citeauthor{risse2014documenting}).

Scholars who are in favor of archiving Twitter argue that not only the text of a tweet is important, but also the context, like who sent the tweet and at what time. They also argue that further natural language processing is needed to search an archive for specific topics. This necessary natural language processing is no easy task on Twitter. Due to the 140-character-limit (which was doubled to 280 in recent years), many vernaculars and abbreviations are used, which can be difficult for natural language processors to process. Nevertheless, the authors claim that \say{Twitter can be seen as a comprehensive documentation of society} \parencite[9]{risse2014documenting}, and say that this documentation can be of immense historical value for future generations.

\subsection{Twitter as news source}  % Maybe "Twitter and crowd journalism" or something?
When Osama bin Laden was killed, the information was spread via Twitter before it hit major news outlets (\cite{hu2012breaking}). \citeauthor{hu2012breaking} analyzed the certainty of tweets that were sent between the first rumors of the killing and the official confirmation. This certainty analysis showed that many people were already sure that this news was true before officials confirmed it. \citeauthor{hu2012breaking} concluded that people saw Twitter as a trustworthy source for this breaking news. The bin Laden killing is thus one of the earliest reported cases of Twitter being faster than traditional media, but equally accepted as a trustworthy source (\cite[2751]{hu2012breaking}).

A study examining the social media-usage around the protests of the 2010 G20 summit in Toronto shows that Twitter is a suitable platform for crowd-sourced journalism (\cite{poell2012twitter}). Crowd-sourced journalism, or alternative journalism, positions itself as the opposite of mainstream journalism. According to the authors, mainstream journalism often focuses on the events themselves, like demonstrations or riots, instead of the underlying issues that people are protesting against. Opposed to that, crowd journalism has the chance to shine a light on why people are protesting, by making these people heard (\cite[698]{poell2012twitter}). 

This study from 2012 analyzed the usage of three different social media platforms during the G20 protests: Twitter, YouTube, and Flickr. Of the three platforms the researchers examined, Twitter was used by most people, and the retweeting-feature gave more silent users the possibility to weigh-in without having to create their content (cf. \cite[709]{poell2012twitter}). However, the authors concluded that the content published on social media-platforms was focused on sensationalist messages, similar to how mainstream journalism operates. Instead of focusing on the meaning of the protests, and the motivation of those protesting, the focus lay again on sensationalist pictures. This goes against the intention of crowd-sourced journalism.

Using social media in as a reliable source for news and knowledge even affected the 2016 election of the US president (\cite{duttSenatorWeSell2018}, \cite{ribeiroMicrotargetingSociallyDivisive2019}). During this election, Russia allegedly bought microtargeted ads on Facebook which appeared on users' newsfeeds as regular posts with a small tag claiming that this was paid content. These paid ads often contained false information about the democratic party and their candidate, Hillary Clinton. Researchers came to the conclusion that these ads, specifically targeting key demographics which could have swung the vote against Trump, tampered with the outcome of the election in Russia's favor.

\subsection{Filter bubble in social networks}
While social networks can connect people all over the world, scientists believe that their recommender algorithms are designed in a way that amplifies users' own opinions instead of connecting users with opposing views to generate healthy discussions (\cite{pariser2011filter}). \citeauthor{pariser2011filter} coined the term of the \emph{filter bubble}. His thesis is those recommender algorithms select content for users that they mostly agree with. This traps users in a bubble where they are rarely confronted with opposing world views, as Google searches, the Twitter timeline, Facebook, Instagram, Reddit, and the likes hide those contents---without the users' consent or even knowledge.

This leads to worries about online discussions becoming less diverse. Rather than being exposed to a plethora of viewpoints and arguments, social media platforms and search engines decrease information diversity to improve engagement on their websites (\cite{bozdagBreakingFilterBubble2015}). One study found out that especially in political issues Twitter users tend to communicate mainly with users who share their political views (\cite{barberaTweetingLeftRight2015}). In a filter bubble, the own opinion seems to be shared by the majority of people---even if one is part of a vocal minority.

How big the effect of the filter bubble on the culture of discourse is remains unclear (\cite{brunsEchoChamberWhat2017}). A study examining the Australian Twitter-sphere has found that while there are some clusters of Twitter users, \say{those connections have not been made to the exclusion of all others} (\cite[9]{brunsEchoChamberWhat2017}). This means that while there are distinct groups of users that are all strongly connected, those clusters are still connected. Users from these clusters have connections to people \emph{outside} their cluster: from a network-centric view, this indicates that filter bubbles don't exist, as distinct clusters would have little to no connections between each other. However, this study fails to incorporate Twitter's default news feed which shows algorithmically picked posts first rather than a chronological timeline of all activity in a network. Even though people follow other people they do not necessarily agree with, the recommender algorithm could only show tweets by others they agree with.

One survey conducted in the UK showed that politically interested users know about echo chambers and actively try to avoid them (\cite{duboisEchoChamberOverstated2018}). The participants of this study reported that they actively choose to read some media outlets which they know they do not agree with. \citeauthor{duboisEchoChamberOverstated2018} conclude that people who are politically interested and have a diverse media diet can prevent themselves from getting caught in a filter bubble and thus keep a broader look on ongoing discussions. Seeing that the results come from a survey where participants self-reported their behavior and their internet usage, it is unclear if these users manage to avoid the filter bubble: \citeauthor{bozdagBreakingFilterBubble2015} pointed out that users are usually unaware that their content is filtered. 

The fact that social networks influence people's opinions can become problematic because people can adapt their opinion to match these of opinion leaders (\cite{altafiniDynamicsOpinionForming2012}). \citeauthor{gokceTwitterPoliticsIdentifying2014} describe opinion leaders in the following way: 
\begin{quote}
    (Opinion leaders are people) whose ideas have a continuous presence and following in conventional, and—more recently—in electronic media. Newspaper columnists, non-governmental organization leaders, prominent politicians, and public intellectuals (\emph{and sometimes popular media ﬁgures}) fall into this category (\cite[673]{gokceTwitterPoliticsIdentifying2014}, emphasis by the author.)
\end{quote}

Adapting one's opinions to those of popular media figures may mean to accept someone as an authority in a field where he or she has no formal education and is unknowingly spreading false information. The opinion leadership of media figures, paired with the effect of the filter bubble, could lead to a less-informed minority who see themselves as a well-informed majority (\cite{moscoviciSilentMajoritiesLoud1991}).

%It appears as if Twitter is, on the one hand, a grassroots medium, allowing the broader public to engage in discussion and form an opinion

\subsection{Breaking the bubble using big data}
As previously discussed, the filter bubble may prevent people from acknowledging that their own view point is not the universal truth, as well as leading to the false impression that they represent a majority of people. As this effect is mainly driven by the content recommendations algorithms inside the network, and users have little to no control over which content these algorithms show to the individual users, it may be impossible to get a clear, unbiased view as a user from inside the network.

One way to break the bubble is to get a bird's eye view of Twitter. Twitter's API can be used to automatically collect all publicly available tweets that match certain search criteria given by the user (\cite{twitterinc.TwitterAPIs}).

The API allows easy access to tweets and provides functionality to filter tweets by keywords, hashtags, language, or geographic regions (\cite{bello2017detecting}). This allows scientists and coders to get a nearly full overview, unbiased by recommendation algorithms or personal preferences. Twitter offers two different ways to access tweets via their API:

\begin{enumerate}
    \item \textbf{The Archive API} gives access to Twitter's archives up to 30 days. Users can search for tweets matching specific criteria and retrieve them from the archive.
    \item \textbf{The Streaming API} streams new tweets that match given criteria to a specified endpoint as soon as this tweet is sent. New contents immediately reach the endpoint, however, content that has already been sent before the data collection started cannot be retrieved using this API.
\end{enumerate}

Collecting data automatically via Twitter's API is not without its flaws, though. Over the last years, Twitter implemented a premium model for its API; due to the high costs for the premium access, scholarly research on Twitter has been limited (\cite{brunsTwitterDataWhat2014}). One example of the limitations is that the free API plan only gives access to 1\% of the total current volume on Twitter. If 100,000 tweets are sent per minute over all of Twitter, the API would only give access to at most 1,000 tweets matching the query. Using the Streaming API gives access to all tweets matching the given criteria in a given moment; however, access to Twitter's archives is impossible using this stream. This makes it necessary for researchers to set up their own database and collect data over a prolonged time period.

Collecting such big amounts of raw data is commonly called working with \emph{Big Data} (\cite{crawfordCriticalQuestionsBig2012}). This term used to refer to data sets that were so big that you needed a supercomputer to use them; \citeauthor{crawfordCriticalQuestionsBig2012} noticed, however, that the name nowadays carries technological, analytical, and mythological components. Big Data does not only mean bigger data sets, but also carries a 

\begin{quote}
    widespread belief that large data sets offer a higher form of intelligence and knowledge that can generate insights that were previously impossible, with the aura of truth, objectivity and accuracy. (\cite[3]{crawfordCriticalQuestionsBig2012})
\end{quote}

This belief seems to be summed up in the equation 'more data = more insight'.

Seeing that this automated collection generates a huge data set, it is nearly impossible to analyze and categorize the collected data manually. Luckily, the last few years brought big advances in natural language analysis, so this is not a problem anymore. Automated natural language analysis can be used to understand and explain social behavior in text-based social networks.

\citeauthor{bahk2016publicly} developed a prototype for a dashboard that helps to monitor the population's sentiment toward vaccinations. The so-called 'anti-vaxxers' pose a real threat to the wider population because vaccines only work effectively if a majority is vaccinated. Thus, it is interesting to know when and where negative sentiment towards vaccinations arises to formulate countermeasures, like targeted information campaigns. The authors searched Twitter for vaccine-related messages and automatically calculated the sentiment of those tweets. The generated data is presented on a dashboard, which shows the negative sentiment per country, along with further filters. According to the authors, \say{[the dashboard] can be used to detect early signals in shifting conversations about vaccination} (\cite[343]{bahk2016publicly}). Users of this dashboard can see in which regions vaccinations are particularly frowned upon, and can then continue to formulate said countermeasures.

Another study found that Twitter is a useful source for measuring climate change awareness (\cite{codyClimateChangeSentiment2015}). The authors of this study developed a so-called \say{Hedonometer} that they use to compute a happiness-score for tweets. First, they used Amazon's \emph{Mechanical Turk} service to let people rate words in a corpus on a scale from 1--least happy to 9--most happy. Then, they removed neutral or ambiguous words which scored in the middle of the scale, between 4 and 6, from the corpus. This annotated corpus was used to compute the overall happiness of a tweet. With this set of happiness-scores for tweets, \citeauthor{codyClimateChangeSentiment2015} mapped out the average happiness of tweets that contained the word \emph{climate} compared to the average happiness of all collected tweets. This gave the authors an unfiltered insight into how the general public perceives climate change and the surrounding discussions.

\citeauthor{weberInterdisciplinaryOptimismSentiment2019} also collected tweets automatically to find out how researchers experience interdisciplinary research. For this, they collected tweets that contained the words \emph{interdisciplinary}, \emph{transdisciplinary} and \emph{multidisciplinar} using the Twitter API. After they collected the dataset, they pre-processed it to remove duplicated tweets and retweets, as well as lemmatization, removing numbers and punctuation, and other common pre-processing steps for natural language processing. After this, they trained a machine learning classifier using several publicly available annotated datasets. This classifier then categorized tweets in three categories: \emph{positive}, \emph{neutral}, and \emph{negative}.

They found out that negative tweets did not only contain tweets that were critical against interdisciplinary studies, but also tweets that depicted every-day hardships scientists have to struggle with, like rejected peer-reviews and difficulties in securing funding. Neutral tweets were most often job postings, calls for applications, or announcements for new publications. Positive tweets were often about conferences and workshops, but also about the method of interdisciplinary research itself. Most of the tweets collected in this study were classified as neutral (\cite[7]{weberInterdisciplinaryOptimismSentiment2019}).

% Idee dieser Subsection: Erklären, was alles dadurch erreicht werden kann, dass NLA auf Tweets angewandt wird. Beispiele: Vaccination Dashboard, rausfinden dass anti-vaccine tweets häufiger retweetet werden als pro-vaccine tweets

\subsection{Working with big data}

In the last couple of years, we generated more data than in the combined history of humankind (\cite{helbing2019will})---and with the Internet of Things on the rise, experts assume that the amount of data will double every 12 hours.

The more data we collect, the more important it becomes to understand big data (\cite{bornerDataVisualizationLiteracy2019}). The most common form to store such big data is in databases, which usually show data in big tables. However, data in tabular form is exceptionally hard to read for humans. As human perception is fine-tuned to recognize underlying patterns, trends, and outliers, \emph{Data Visualization} is one of the most common tools to explore and communicate big data sets (\cite{heerTourVisualizationZoo2010}). Recognizing trends helps with understanding what the data set is telling in general, while outliers help to find cases that could require further attention and detail.

Visualizations help to make sense of big data in a more intuitive way than reading database tables or big charts (\cite{donalekImmersiveCollaborativeData2014}). As data-driven decision-making becomes more and more important (\cite{brynjolfssonStrengthNumbersHow2011}), the ability to read and construct data visualizations is going to become an important skill in the future.

But even though data visualizations are known to help people understanding big data sets, studies have shown that laymen often do not have the required knowledge to accurately read complex visualizations (\cite{bornerInvestigatingAspectsData2016}). This means that data visualizations have to be constructed carefully and assume almost no previous knowledge when it comes to explanatory texts or supporting elements in the visualizations. As \citeauthor{bornerInvestigatingAspectsData2016} have shown, laymen can read the most basic kinds of data visualizations, like bar charts and line charts. However, even for those well-known charts, participants had trouble naming and interpreting the visualizations.

Thus, data visualization is not inherently self-explanatory. Our ability to extract information from visualizations and answering questions based on data sets is called \emph{Data Literacy} (\cite{boyPrincipledWayAssessing2014}). Improving this literacy allows people to not only improve their communication and collaboration on data sets, but also their understanding of the world as shown by big data (\cite{bornerDataVisualizationLiteracy2019}). 

\begin{itemize}
    \item A good data visualization does not provide definite answers in itself; rather, it should support the users in the search for results (\cite{light2001portable}).
    \item Data visualizations should be carefully constructed so it does not overwhelm the user. Shneiderman suggests: overview first, then zoom and filter, lastly details-on-demand (\cite{shneidermanEyesHaveIt1996}). By giving an overview of the data set, the context in which the data (and thus the visualization) lies becomes clearer. Zooming and filtering allows users to choose a portion of the data set that they are especially interested in. Details-on-demand allows them to gain additional insight in specific data points. Giving those details on demand, rather than always showing them, makes the visualization less cluttered and more easily understandable.
    \item Measuring the insight that users gain from data visualizations is hard (\cite{northMeasuringVisualizationInsight2006}). One problem is that \emph{insight} is hard to define. \citeauthor{northMeasuringVisualizationInsight2006} gives five key characteristics to insight:
    \begin{enumerate}
        \item \textbf{Complex,} which means that not only single data points are taken into account to measure insight, but all or at least most of the given data
        \item \textbf{Deep,} meaning that \say{insight often generates further questions} (\cite[6]{northMeasuringVisualizationInsight2006}) that in turn generate new insight
        \item \textbf{Qualitative,} meaning that the insight is not, e.g., an exact number, but more of an overall feeling
        \item \textbf{Unexpected,} with the author saying that insight is often creative and unpredicted, and
        \item \textbf{Relevant,} meaning that the insight goes beyond 'number-crunching' towards relevant results that affect the domain
    \end{enumerate}
    To measure these five dimensions, the author proposes to drop controlled experiments with benchmark tasks. These experiments often require very precise instructions for the participants which do not test insight very well as those tasks often result in yes/no-answers. Instead, \citeauthor{northMeasuringVisualizationInsight2006} says that complex benchmark tasks that cannot be answered with yes or no help researchers better measure insight. One step further, benchmark tests could be eliminated, by focusing on qualitative analysis. For this, participants could be asked open-ended questions without giving them instructions on how to find specific data in the visualization. While the users solve the task, they are asked to think aloud. Their utterances can then be coded by the interviewer to map then on the five dimensions discussed above.
\end{itemize}
