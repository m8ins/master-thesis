\section{Discussion}
This section will discuss the findings from the interview study. For problems that were mentioned in the user tests, a possible solution will be discussed. For motivational factors, ways to further improve these factors will be taken into consideration.

\subsection{Hindering factors: missing interface components}
In this subsection, problems that can be solved with additional interface components will be discussed.

\subsubsection*{Overview of currently set filters}
Multiple participants had trouble recognizing what data filters were applied for the current visualization. The idea that the filters can be changed on top of the notebook and will then influence the rest of the visualization does not seem to work, especially on smaller laptops, as this resulted in a lot of scrolling.

\begin{figure}[htbp]
    \fbox{\includegraphics[width=\linewidth]{images/amazon_filters.jpg}}
    \caption{The Amazon search is filtered to English-language books which are eligible to prime-shipping, with the search input \emph{don't make me think}}
    \label{fig:amazon_filters}
\end{figure}

This means that the filter status should be visible directly next to the visualization. This way, participants would not need to search for the current filter status. Rather, the status would almost always be in view while interpreting the visualizations. There are two possible ways to show the filters near the visualizations:
\begin{enumerate}
    \item \textbf{Show a descriptive text above the visualization:} using a text to show the status of the filter could help participants to quickly understand what kind of data the visualization is showing at the moment. One example of this technique can be found on Amazon, as seen in figure \ref{fig:amazon_filters}. This technique is especially useful when a lot of different filters can be applied.
    \item \textbf{Show the filter toggles right next to the visualization:} instead of showing the filter status as a text, showing them with the actual toggles would allow users to change the filters right next to the visualization.
\end{enumerate}

\begin{figure}[h!tbp]
    \fbox{\includegraphics[width=\linewidth]{images/legend_and_filters.png}}
    \caption{Proposed layout of the tweet volume with a legend and filters directly next to the graph}
    \label{fig:legend_and_filters}
\end{figure}

Showing the filter toggles right next to the visualizations seems like a straightforward idea. However, this raises the question if the filters of the two visualizations should be linked or not, i.e., should the filter of the sentiment graph change as well when the filter of the volume graph is changed or not. To decide this, further studies should be conducted to see if the synchronicity of the data visualizations is helpful or not. This study did not examine if it helps users to have a single filter context with several visualizations or not. During the exploration phase in the interview, however, the participants did not seem to switch a lot between the tweet volume and the sentiment chart. This could suggest that having a single filter applied to both visualizations is not a necessity. In this case, using two filter sets that are independent of each other could avoid user confusion as only explicit user actions influence the visualization.

\subsubsection*{Color legend}
Another missing interface component are colored legends. During user testing, several problems arose because of the missing legend. For one, the graph was less self-contained. Users were forced to read the explanatory texts that accompanied the graph to be able to understand what the different colors mean. Another problem with the textual explanation of colors was that different participants would have named the colors differently than the text did. A legend like the one shown in figure \ref{fig:legend_example} could have helped identifying the colors more easily, as well as making the graph more self-explanatory.

Figure \ref{fig:legend_and_filters} shows a proposed addition to the tweet volume-graph. Implementing these changes makes the current filter status more obvious and lets users toggle the different filters without needing to scroll to the top of the notebook again. The legend with dynamic labeling, which includes the search term the user entered, makes the graph more self-contained.

The added legend could also help solve the layout problem that participants mentioned. With the explanatory texts no longer necessary to understand the graphs, the texts serve as an additional explanation and, as such, can remain below the graph itself.

\subsubsection*{Loading indicator}
Another component that was missing from the interface was an indicator that new data was fetched from the server. One possibility to show this fetching would be to show a small loading indicator next to the word filter. This, however, could be not visible enough. Another possibility would be to show in the visualizations themselves that new data is being fetched. As fetching takes about 5 seconds and the visualizations have not been updated before the fetching is complete, it should not be a big problem to visually hide the visualizations during the fetching stage. A possible solution for this can be seen in figure \ref{fig:fetching_state}.

\begin{figure}[htbp]
    \fbox{\includegraphics[width=\linewidth]{images/fetching_state.png}}
    \caption{Proposed solution to show that new data is being retrieved from the server}
    \label{fig:fetching_state}
\end{figure}

\subsubsection*{Tooltips for the sentiment graph}
During the user tests, the participant expected to find tooltips for both graphs once they found the tooltip in the volume graph. This means that tooltips should either be used globally for all visualizations---at least where the additional information provided by the tooltip adds to the value---or not at all. As the tooltip in the volume graph helped participants to further explore 

\subsection{Hindering factors: Technical limitations}
This section discusses findings from the interview where technical limitations hindered participants from solving tasks. Some of these limitations were conscious trade-offs for this prototype version. Nonetheless, these issues should be addressed if a tool based on this prototype should be further developed.

\subsubsection*{Fixed data set}
The fixed data set came up in one interview where the participant tried to scroll the data set to reveal even more days. This revealed one of the biggest disadvantages of using Twitter's streaming API to collect tweets in real-time (see chapter \ref{sec:fetchedData}). Using this method guarantees the most complete data set on the free tier of Twitter's API, at least from the time the data collection is started. The downside is that using this API makes it impossible to access past tweets. Users of the tool have to recognize a potentially interesting topic very early, start the data collection, and then wait some time until enough data has been gathered.

Using the paid API to access Twitter's archives could solve this problem, at least in parts. The paid access to Twitter's archives allows scanning the past 30 days of activity on Twitter, which could give a 'headstart' to the data collection. For a final version of this tool, a split approach could be possible: using the real-time data collection of a specific topic using the streaming API, and simultaneously fetching the last 30 days of Twitter activity to this topic. This would mean that users would get direct access to the past month of tweets directly from the beginning of the data collection, eliminating the need to wait some days before they can gather meaningful results from Twitter.

\subsubsection*{Word filter instead of fuzzy search}
Another problem that needs further technical work is how the word filter operates. The current implementation acts as a case-insensitive filter that looks for parts of words. This means that, if a user enters \emph{App}, tweets containing these sorts of texts will be found:

\begin{itemize}
    \item I like this \textbf{App} a lot
    \item Whats\textbf{App} collects your data
    \item What kind of video \textbf{app} would you recommend?
    \item I'm not angry, just dis\textbf{app}ointed
\end{itemize}

This is different from how search engines, like DuckDuckGo, Google, or Bing, work. These engines use a so-called \emph{fuzzy search} which not only looks for the word itself, but also catches typos, looks for synonyms, and so on.

Using a fuzzy search would allow users to explore the database more efficiently. During testing, for example, typos would mean that little to no results were found. Participants thought that there was not a lot of Twitter activity about the specific topic they were interested in. After one participant was told to correct the typo, she noticed an interesting pattern in the tweet activity and was able to interpret the data further. This means that it could be helpful to implement a fuzzy search if the prototype should be further developed.

\subsection{Hindering factors: missing information}
This section is about pieces of information that participants said they would expect to interpret the data more efficiently. 

\subsubsection*{Missing real-world context}
As discussed in chapter \ref{sec:fetchedData}, one idea was to give participants some context as to what happened on specific days based on the tweet-texts themselves. Because the calculated collocations did not yield satisfying results, this feature was not implemented for the prototype of the dashboard.

Some participants explicitly said that they would have liked to know what happened on specific days, especially when they recognized spikes in Twitter activity for a topic or spikes in the sentiment on a specific day. Further development of the tool should take this into account and, e.g., collect daily Twitter trends using the Trend API\footnote{https://developer.twitter.com/en/docs/twitter-api/v1/trends/trends-for-location/api-reference/get-trends-place, last visited on 24.09.2020}. This would allow participants to analyze the dataset more thoroughly. In several interviews participants said that they would need more context to understand spikes in the tweet volume or the sentiment graph. They expected this information to be present in the prototype itself, and no participant searched on the internet for further information on what happened on these days. The Twitter trends could give at least some hints on what might have happened on these days.

Showing participants the trending topics for specific days could also give them more ideas for topics they could search for. During the free exploration in the interviews, some participants immediately had ideas that they wanted to check in the dataset (see, for example, w26b, l. 9: \say{Ich probier es mal mit Stuttgart denn da gab es ja glaub ich eine der ersten dieser Hygiene-Demos}). Others only explored the data set that was open by default, which was filtered with \emph{Corona}. Showing daily trends could give participants ideas for how to explore the dataset further.

\subsubsection*{Missing examples for the sentiment graph}\label{sec:missingExamples}
Participants said that they found the sentiment analysis interesting. Some wished for further explanation of how the sentiment is computed and also wished to see examples of Tweets with a sentiment of +1 and a sentiment of -1 (see m23, l. 73: \say{Also klar, was das irgendwie bedeuten soll, aber... wie sieht jetzt zum Beispiel ein positiver Tweet aus oder ein negativer.}).
The same participant also said that he would have liked some advice on how to correctly interpret the sentiments, as well as potential pitfalls for the analysis that he should take care of. This explanation could be done in a similar form as Google explains the sentiment analysis in their Natural Language API Demo\footnote{https://cloud.google.com/natural-language/natural-language-api-demo, last visited on 24.09.2020}, as seen in figure \ref{fig:sentiment_google}. According to the participants, they would not want such an explanation for every single tweet, but rather for selected examples. This means that these examples could be hard-coded in the interface and would not have to be dynamically generated from the dataset.

\begin{figure}[htbp]
    \fbox{\includegraphics[width=0.8\linewidth]{images/sentiment_google.jpg}}
    \caption{The demo shows the sentiment of the full text, of the complete sentences, and for individual entities in the text.}
    \label{fig:sentiment_google}
\end{figure}

At the same time, other participants said that a lengthy explanation of how the algorithm works would not have helped her and that she preferred not to have additional explanations. This means that an explanation should be an optional interface component, e.g., a tooltip near the sentiment graph. This allows interested users to access the information, while users who do not want to learn more about sentiment analysis can focus on interpreting the data at hand.

\subsubsection*{Neutral tweets-area was not marked as such}
When asked to interpret the sentiment graph, multiple participants focused on the threshold of ±0.3 for neutral tweets. This threshold was mentioned for the explanation of the \emph{neutral Tweets}-toggle and is, as discussed in chapter 2, a common threshold to differentiate between neutral Tweets and Tweets that carry an opinion. The participants looked for peaks above or below ±0.3 to find days where the discourse on twitter was especially emotional. This could be facilitated by marking this area in the chart itself, similar to how the 0-baseline was marked. 

\begin{figure}[htbp]
    \fbox{\includegraphics[width=\linewidth]{images/sentiment_neutral_area.jpg}}
    \caption{The area containing the neutral tweets is marked, making it easier to identify spikes above or below the threshold. The screenshot shows the data when filtered for \emph{Drosten}.}
    \label{fig:sentiment_neutral_area}
\end{figure}

\subsection{Motivational factors}
This section discusses motivational factors that were found during testing and possible ways to further strengthen these points in future developments of the tool.

\subsubsection*{Easily understandable visualizations}
The participants said that the visualizations were easily understandable. Future development should keep in mind that the ability to quickly understand the visualizations motivated the participants to discuss the results further, rather than being preoccupied trying to understand what the visualization tries to show them.

To keep the visualizations easily readable, the paradigm \emph{One job per visualization} should be followed. The clear separation of concerns between the two visualizations seemed to support the participants when answering questions. 

If new visualizations should be added to the tool, this should be done in a separate chart, rather than by adding new elements to one of the existing charts.

The separation of the visualizations could be deepened even further by showing the different visualizations in different tabs. During the user tests, the participants rarely referred to the other visualization during interpretation.

\subsubsection*{Supporting explanatory texts}
The explanatory texts helped the participants understanding the visualizations and the capabilities of the tool better. As already discussed in chapter \ref{sec:missingExamples}, these explanatory texts can be further improved, e.g., by adding examples and hints for analysis.

One thing that could be changed about the explanatory texts is their position. Due to the nature of Observable, which was used to build the prototype, explanatory texts were always visible and did not only show in context. One way to further improve the way the texts are presented is by visually hiding some of the texts in the interface. For this, texts should be divided into two kinds:
\begin{enumerate}
    \item Texts that are essential to understanding the visualizations
    \item Texts that offer additional help, insights, explanations, or inspiration for analyzing the dataset
\end{enumerate}

Dividing texts into these two categories could yield a less cluttered interface. Important texts from the first category can be always shown, while texts that fall into the second category can be hidden, e.g., in tooltips or detail panes. Showing only necessary texts also means that the visualizations are front-and-center in the tool. It would also eliminate the need to scroll a lot when accessing the tool from a laptop or tablet (cf. m26, l. 75).
