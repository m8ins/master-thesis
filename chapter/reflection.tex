\section{Method reflection}
This section will discuss the methods used for this master thesis. While the results are generally convincing, there is still room for improvement in further studies.

\subsection*{Using SQL databases instead of NoSQL}
As discussed in chapter \ref{sec:storage}, one of the earliest decisions while working on this thesis was whether to use a NoSQL database, like Firebase, or an SQL database. Both of these solutions have their advantages and drawbacks. NoSQL databases are generally easier to query, as their query syntax often uses method chaining for filtering and querying. This means that the syntax can closely follow how the query would be described verbally. One could, e.g., want to see all tweets by verified accounts, but only those that have a sentiment between +0.3 and +1. Using method chaining, the query could be something like the following:

\begin{verbatim}
    data.filter('is_verified', '==', 'true').filter('sentiment', '>=', '+0.3')
\end{verbatim}

The drawback is that NoSQL databases can become slow when they need to aggregate over large amounts of data. This is one of the core strengths of SQL databases, as they remain performant even with big data sets. SQL also uses a fixed table scheme, which helps to keep the data in a strict form as is needed for the interactive visualizations. However, as they work with the principle of relational algebra, SQL queries are harder to write.

Seeing that the final dataset collected throughout the study was more than 1GB big, SQL seemed to be the right choice, even if the queries were more complicated to write.

\subsection*{Using qualitative content analysis could only answer parts of the research question}
One of the biggest issues for the author of this study is that the focus of the study and the focus of this written-out thesis do not fully overlap. The study began as a feasibility analysis that aimed to find out if it is possible to collect large amounts of tweets and show them in such a way that laymen can interpret the data. In this initial state, the study consisted of two parts:

\begin{enumerate}
    \item The automated collection and storage of every German tweet to a specific topic---in this case, tweets related to the Coronavirus,
    \item and an easily accessible visualization of these tweets which can give insight to people who are laymen when it comes to analyzing big data
\end{enumerate}

One of the study's goals was to test several approaches on how to implement these two parts. To evaluate the outcome of this study from a communications science-perspective, an additional interview study based on Mayring's approach to qualitative content analysis was conducted. The results reported in this thesis are mostly based on the findings of this interview study.

However, this study did not further examine the quality and feasibility of the code that was written. While the study showed that it is possible to automatically collect and store tweets with certain filter criteria, it did not give additional insight in \emph{how} to do it. Nevertheless, the insights that were generated from the interview study were valuable.

\subsection*{Using the Streaming API made it impossible to take Likes into account}
One of the possible problems of collecting the data the way they were collected is that Likes could not be taken into account for the analysis of the tweets. Studies have shown that Likes are strongly linked to actual engagement and support, e.g., during congressional elections (\cite{macwilliamsForecastingCongressionalElections2015}). This hints at the importance of Likes to measure the general public's approval of specific tweets. In the current implementation, the volume of the tweets strongly affects the sentiment analysis: if many negative tweets are sent on a specific day, the overall sentiment on that day will go down. This holds true the other way around as well. Many positive tweets affect the curve so that the daily average goes up.

This means that, in theory, a bot army could flood specific topics with very negative tweets. Even if not a single person saw these tweets, or interacted with them, these tweets would affect the reported overall sentiment. If the Likes of a tweet could be collected as well, users could, e.g., filter for tweets with at least 5 likes. This would filter out the tweets which did not drive engagement on Twitter.

On the other hand, this approach would bring additional problems:
\begin{itemize}
    \item Tweets from smaller accounts would be filtered out, even though these accounts are not necessarily part of a bot network.
    \item Filtering out tweets that have less than an arbitrary number of Likes distorts the data set in some way. While they might paint a more accurate picture of what society thinks by filtering out automated, nearly invisible tweets, this could also silence the influence of people without celebrity or influencer status. Seeing that Twitter is a grassroots medium that allows people to make their voices heard regardless of their social status (\cite{passmann2019alte}), a carelessly implemented filter option could silence those voices.
    \item Collecting a tweet's Likes requires either the usage of the archive API or a script that fetches the Likes of a tweet a specific time after it was collected, e.g., three days after initial collection. This is necessary because the streaming API sends a tweet to the collecting script almost immediately after it was sent. This means that every time the streaming API sends a tweet, its number of Likes should be 0.
\end{itemize}

Future studies could focus on addressing these issues and finding a way to make the dataset, on the one hand, more reliable---e.g., by filtering out bots---while on the other hand keeping the open culture of Twitter in mind and not silencing the voices of everyday people.

\subsection*{The prototype does not follow accessibility guidelines}
The prototype created in this study does not follow the web content accessibility guidelines by the W3C(\cite{iso2iec}). One of the most prevalent problems is that the visualizations rely too much on color alone to convey information. This makes visualizations potentially inaccessible to colorblind people.

There are two ways to improve this situation:

\begin{enumerate}
    \item The visualization showing the tweet volume over time could incorporate texture in addition to color to differentiate between the two groups 'Tweets containing the search word' and 'All tweets'. Seeing that both color and patterns are qualitative nominal variables, they can be used interchangeably to encode the same information (\cite{bornerDataVisualizationLiteracy2019}). %TODO in Figma prototypen wie das aussehen kann?
    \item The visualization showing the daily average sentiment cannot use a pattern to additionally encode the colors because the line is too narrow to show a pattern. In this case, colors with sufficient differences in contrast and luminance should be chosen. In addition to this, the afore-mentioned visual color legend should be incorporated, as the textual explanation could not sufficiently explain the colors as they are perceived by a colorblind person.
\end{enumerate}

Incorporating these changes would make the tool more accessible.

\subsection*{Using Shape Up to write a master's thesis} \label{sec:shape_up}
The big data pipeline, the visualization dashboard, and the user studies were all planned and executed using the principles of \emph{Shape Up}, a relatively new management approach for software development projects (\cite{singerShapeStopRunning2019}). The core principle of Shape Up is to have a fixed time frame with a variable scope. Instead of creating a set of user stories, which then get estimated on how long they probably take, Shape Up's philosophy is to set clear boundaries, take a fixed amount of time (usually six weeks), and make the best use of this time. This methodology seemed like a more reasonable approach to writing a prototype for a master's thesis as the time frame is very strict.

Using Shape Up worked well and ensured that the initially defined schedule was followed until the end. In hindsight, though, admitting as much time to building the Big Data-pipeline as to researching and building the interactive visualizations was questionable. Due to the principle \emph{fixed appetite, variable scope}, the full six weeks that were planned for completing the pipeline were used. A first working, albeit a rough one, was already running on the   servers after four weeks, so that the additional two weeks were used to reduce complexity in the code and make it a bit more stable. These two extra weeks could have been helpful to enhance the two visualizations that were built the following six weeks. Nevertheless, Shape Up facilitated meeting the self-set deadlines.

\subsection*{Homogenous group of test participants}
The participants who took part in the user tests were overall a homogenous group, as they were all students aged 23 to 26. Some of them used Twitter, but they only rarely posted own content there. This could have made it difficult for some participants to interpret the visualizations, as domain knowledge is an important part of gaining insights from visualizations (\cite{northMeasuringVisualizationInsight2006}). Future studies should focus on recruiting participants from more diverse backgrounds and should try to find people who use Twitter more actively than the participants from this study.

\subsection*{Using Observable as a prototyping platform}
Using Observable sped up the development process and was the right choice for the prototype. For a final product, however, the limitations of Observable are too severe, especially when using databases. Notebooks with databases require permissions to be set up, and for this users need to create an Observable account and join the team workspace where the notebook is located. Instead of using Observable, users should be able to use a web tool without logging in. One example of a data analysis tool on the web is \emph{Blacklight}\footnote{https://themarkup.org/blacklight/}, where users can scan other web pages for trackers.

\subsection*{Leaving out the effect of irony}
Irony and sarcasm were not at all taken into account in this study. While scholars are looking into detecting sarcasm using natural language processing (\cite{reyesMultidimensionalApproachDetecting2013}, \cite{barbieriModellingIronyTwitter2014}), these models have not been incorporated in NLTK. This could have skewed the sentiment chart towards positivity. As irony works by saying something norm-adhering that, through context, gets recognized as meaning the opposite---and thus norm-breaking---many ironic utterances are in a surface-literal meaning positive (\cite{gioraIronyNegation1995}). This means that they would most likely get a positive sentiment score, while their intended meaning is negative. The problem of irony was consciously left out of the prototype to avoid confusing the participants. Future work should re-evaluate whether to incorporate irony detection during the language processing step, if this topic should be explained in an explanatory text, or if it should be still left out.
