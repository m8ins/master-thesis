\section{Method reflection}
This section will discuss the methods used for this master thesis. While the results are generally convincing, there is still room for improvement in further studies.

\subsection*{Using qualitative content analysis could only answer parts of the research question}
One of the biggest issues for the author of this study is that the focus of the study and the focus of this written-out thesis do not fully overlap. The study began as a feasibility analysis that aimed to find out if it is possible to collect large amounts of tweets and show them in such a way that laymen can interpret the data. In this initial state, the study consisted of two parts:

\begin{enumerate}
    \item The automated collection and storage of every German tweet to a specific topic---in this case, tweets related to the Coronavirus,
    \item and an easily accessible visualization of these tweets which can give insight to people who are laymen when it comes to analyzing big data
\end{enumerate}

One of the study's goals was to test several approaches on how to implement these two parts. To evaluate the outcome of this study from a communications science-perspective, an additional interview study based on Mayring's approach to qualitative content analysis was conducted. The results reported in this thesis are mostly based on the findings of this interview study.

However, this study did not further examine the quality and feasibility of the code that was written. While the study showed that it is possible to automatically collect and store tweets with certain filter criteria, it did not give additional insight in \emph{how} to do it. Nevertheless, the insights that were generated from the interview study were valuable.

\subsection*{Using the Streaming API made it impossible to take Likes into account}
One of the possible problems of collecting the data the way they were collected is that Likes could not be taken into account for the analysis of the tweets. Studies have shown that Likes are strongly linked to actual engagement and support, e.g., during congressional elections (\cite{macwilliamsForecastingCongressionalElections2015}). This hints at the importance of Likes to measure the general public's approval of specific tweets. In the current implementation, the volume of the tweets strongly affects the sentiment analysis: if many negative tweets are sent on a specific day, the overall sentiment on that day will go down. This holds true the other way around as well. Many positive tweets affect the curve so that the daily average goes up.

This means that, in theory, a bot army could flood specific topics with very negative tweets. Even if not a single person saw these tweets, or interacted with them, these tweets would affect the reported overall sentiment. If the Likes of a tweet could be collected as well, users could, e.g., filter for tweets with at least 5 likes. This would filter out the tweets which did not drive engagement on Twitter.

On the other hand, this approach would bring additional problems:
\begin{itemize}
    \item Tweets from smaller accounts would be filtered out, even though these accounts are not necessarily part of a bot network.
    \item Filtering out tweets that have less than an arbitrary number of Likes distorts the data set in some way. While they might paint a more accurate picture of what society thinks by filtering out automated, nearly invisible tweets, this could also silence the influence of people without celebrity or influencer status. Seeing that Twitter is a grassroots medium that allows people to make their voices heard regardless of their social status (\cite{passmann2019alte}), a carelessly implemented filter option could silence those voices.
    \item Collecting a tweet's Likes requires either the usage of the archive API or a script that fetches the Likes of a tweet a specific time after it was collected, e.g., three days after initial collection. This is necessary because the streaming API sends a tweet to the collecting script almost immediately after it was sent. This means that every time the streaming API sends a tweet, its number of Likes should be 0.
\end{itemize}

Future studies could focus on addressing these issues and finding a way to make the dataset, on the one hand, more reliable---e.g., by filtering out bots---while on the other hand keeping the open culture of Twitter in mind and not silencing the voices of everyday people.

\begin{itemize}
    \item Accessibility issues: too much reliance on colors alone. It would have been better to also include texture patterns. Both color and patterns are qualitative nominal variables, so they can encode the same information (\cite[1860]{bornerDataVisualizationLiteracy2019}).
    \item Using \emph{Shape Up} to plan the development of the tool worked well. It is possible, however, that the clear distinction pipeline---d3-visualizations---writing the thesis meant that some parts of the study, like the pipeline, took more time than necessary. At the same time, some features that were planned could not be properly implemented in the given time frame. These features are discussed in further detail in Section \ref{fw_tooltips}.
    \item The participants in the interview study were mostly students. This was partly because of the ongoing Covid-19 pandemic, which made it necessary to use online conferencing software to recruit the participants and conduct the interviews with them. Face-to-face interviews with a more diverse group of participants can yield further results. This should be considered for future studies.
    \item Using Observable sped up the development process and was the right choice for the prototype. For a final product, however, the limitations of Observable are too severe, especially when using databases. Notebooks with databases require permissions to be set up, and for this users need to create an Observable account and join the team workspace where the notebook is located. Instead of using Observable, users should be able to use a web tool without logging in. One example of a data analysis tool on the web is \emph{Blacklight}\footnote{https://themarkup.org/blacklight/}, where users can scan other web pages for trackers.
    \item Irony and sarcasm were not at all taken into account. While scholars are looking into detecting sarcasm using natural language processing (\cite{reyesMultidimensionalApproachDetecting2013}, \cite{barbieriModellingIronyTwitter2014}), these models have not been incorporated in NLTK. This could have skewed the sentiment chart towards positivity. As irony works by saying something norm-adhering that, through context, gets recognized as meaning the opposite---and thus norm-breaking---many ironic utterances are in a surface-literal meaning positive (\cite{gioraIronyNegation1995}). This means that they would most likely get a positive sentiment score, while their intended meaning is negative. The problem of irony was consciously left out of the prototype to avoid confusing the participants.
\end{itemize}
